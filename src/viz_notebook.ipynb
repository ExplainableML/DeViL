{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from model import Features2WordsModel, TranslationTransformerConfig\n",
    "import os\n",
    "from argparse import Namespace # needed for reading saved argparse parameters\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from evaluation import get_saliency_map\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO DEFINE\n",
    "MODEL_CKPT = # path to model checkpoint\n",
    "IMG = '../assets/catdog.png'\n",
    "QUERIES = ['cat', 'dog', 'animal']\n",
    "LAYER = -1 # e.g. -1; if left None will compute metrics with features for all layers.\n",
    "HRES = False # set to True if you want to generate saliency maps for a given layer but with the resolution of the lowest layer.\n",
    "POOL_LOCS = 'keep_dims' # choose between None, reduce_dims, keep_dims. Pools lower layer locations.\n",
    "KERNEL_SIZE = -1 # kernel size for pooling lower layer locations. We typically use 3 for layer -2 and 7 for layer -3.\n",
    "GPU_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "translation_config = TranslationTransformerConfig.from_json_file(os.path.join(MODEL_CKPT, 'translation_model_config.json'))\n",
    "lm_model_config = AutoConfig.from_pretrained(os.path.join(MODEL_CKPT, 'lm_model_config.json'))\n",
    "max_length = lm_model_config.max_length\n",
    "\n",
    "with open(os.path.join(os.path.join(MODEL_CKPT, '../train_params.txt')), 'r') as f:\n",
    "    namespace_str = f.read()\n",
    "train_args = eval(namespace_str)\n",
    "vision_backbone = train_args.vision_backbone\n",
    "vision_feat_func = train_args.vision_feat_func\n",
    "vision_feat_layers = train_args.vision_feat_layers\n",
    "language_model = train_args.language_model\n",
    "model = Features2WordsModel(translation_config=translation_config, cnn_name=vision_backbone, vision_feat_func=vision_feat_func, vision_feat_layers=vision_feat_layers, lm_model_name=language_model, max_length=max_length)\n",
    "model_checkpoint = torch.load(os.path.join(MODEL_CKPT, 'model.pt'))\n",
    "model.translation.load_state_dict(model_checkpoint[\"MODEL_STATE\"])\n",
    "model.eval()\n",
    "model.to(GPU_ID)\n",
    "transform = model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(n, dim):\n",
    "  return sorted(list(set(product(range(n), repeat=dim))))\n",
    "\n",
    "def prepare_grid(model, layer, hres, pool_locs):\n",
    "  stride = 1\n",
    "  # prepare spatial locations\n",
    "  gs = model.grid_sizes[::-1]\n",
    "  if layer is None:\n",
    "      layer_id = None\n",
    "      layer_name = 'all'\n",
    "      gs = gs[0] # get the size of the lowest layer the model was trained on\n",
    "  else:\n",
    "      layer_name = str(layer)\n",
    "      layer_id = -layer - 1\n",
    "\n",
    "      if hres:\n",
    "          gs = gs[0] # get the size of the lowest layer the model was trained on\n",
    "          layer_name += '_hres'\n",
    "      else:\n",
    "          if pool_locs == 'reduce_dims':\n",
    "              gs = gs[-1]\n",
    "              stride = int(gs[layer] / gs[-1])\n",
    "          else:\n",
    "              gs = gs[layer]\n",
    "  \n",
    "  loc_ids = grid(gs, 2)\n",
    "  return gs, loc_ids, layer_id, layer_name, stride\n",
    "\n",
    "def get_saliency(model, layer, hres, pool_locs, kernel_size, img, query):\n",
    "    gs, loc_ids, layer_id, layer_name, stride = prepare_grid(model, layer, hres, pool_locs)\n",
    "    loss_saliency, _ = get_saliency_map(model, loc_ids, img.unsqueeze(0), query, layer_id, gpu_id=GPU_ID, hres=hres, pool_locs=pool_locs, kernel_size=kernel_size, stride=stride)\n",
    "    loss_saliency_interp = loss_saliency.view(-1, gs, gs).unsqueeze(dim=1)\n",
    "    loss_saliency_interp = F.interpolate(loss_saliency_interp, tuple(img.shape[1:]), mode='bilinear')\n",
    "    smap_loss = loss_saliency_interp[0].permute(1, 2, 0).cpu().numpy()\n",
    "    smap_loss = -smap_loss\n",
    "\n",
    "    return smap_loss, layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image\n",
    "img = Image.open(IMG).convert(\"RGB\")\n",
    "img_wo_transform = img.copy()\n",
    "img = transform(img)\n",
    "img = img.to(GPU_ID)\n",
    "\n",
    "for i, q in enumerate(QUERIES):\n",
    "    smap, layer_name = get_saliency(model, LAYER, HRES, POOL_LOCS, KERNEL_SIZE, img, q)\n",
    "    plt.figure(i)\n",
    "    plt.imshow(img_wo_transform)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Layer {layer_name}')\n",
    "    plt.imshow(smap, cmap='jet', alpha=0.5)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
